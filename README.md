# Awesome-Video-Agent

A curated collection of papers, benchmarks, codebases, and other learning resources for building multimodal **video agents**. Inspired by the style of [Awesome Think With Images](https://github.com/zhaochen0110/Awesome_Think_With_Images), this repository organizes the latest progress on perception, reasoning, and action in video-driven systems.

## Table of Contents
- [Updates](#updates)
- [Scope](#scope)
- [Resource Overview](#resource-overview)
- [Papers](#papers)
- [Benchmarks](#benchmarks)
- [Datasets](#datasets)
- [Frameworks & Tooling](#frameworks--tooling)
- [Tutorials & Courses](#tutorials--courses)
- [Contributing](#contributing)
- [License](#license)

## Updates
- **2024-08-12**: Repository initialized with structure modeled after Awesome Think With Images.

## Scope
This list focuses on resources that help build agents capable of understanding, reasoning over, and acting upon video streams. Relevant topics include:
- Multimodal large language models (MLLMs) that ingest video inputs.
- Video captioning, QA, and temporal reasoning.
- Embodied or tool-using agents leveraging video perception.
- Planning, alignment, and safety for video-based autonomy.

## Resource Overview
Use the following sections as anchors for contributions:
- **Papers**: Novel architectures, training strategies, or agent designs centered on video understanding.
- **Benchmarks**: Public leaderboards or evaluation suites targeting temporal reasoning or agent capabilities.
- **Datasets**: Curated video corpora for supervised, self-supervised, or reinforcement learning.
- **Frameworks & Tooling**: Open-source libraries, model checkpoints, evaluation scripts, and agent stacks.
- **Tutorials & Courses**: Blog posts, walkthroughs, and lecture series focused on building video agents.

## Papers
_Organize papers chronologically (newest first) with concise summaries and links to code if available._

| Paper | Release Date | Github |
| --- | --- | --- |
| [V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models](https://arxiv.org/search/?query=V-ReasonBench%3A+Toward+Unified+Reasoning+Benchmark+Suite+for+Video+Generation+Models&searchtype=all&source=header) | 2025-11 | [yangluo7/V-ReasonBench](https://github.com/yangluo7/V-ReasonBench) |
| [Reasoning via Video: The First Evaluation of Video Modelsâ€™ Reasoning Abilities through Maze-Solving Tasks](https://arxiv.org/search/?query=Reasoning+via+Video%3A+The+First+Evaluation+of+Video+Models%E2%80%99+Reasoning+Abilities+through+Maze-Solving+Tasks&searchtype=all&source=header) | 2025-11 | [ImYangC7/VR-Bench](https://github.com/ImYangC7/VR-Bench) |
| [Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm](https://arxiv.org/search/?query=Thinking+with+Video%3A+Video+Generation+as+a+Promising+Multimodal+Reasoning+Paradigm&searchtype=all&source=header) | 2025-11 | [tongjingqi/Thinking-with-Video](https://github.com/tongjingqi/Thinking-with-Video) |
| [RETHINKING VISUAL INTELLIGENCE: INSIGHTS FROM VIDEO PRETRAINING](https://arxiv.org/search/?query=RETHINKING+VISUAL+INTELLIGENCE%3A+INSIGHTS+FROM+VIDEO+PRETRAINING&searchtype=all&source=header) | 2025-10 | [PabloAcuaviva/visual-intelligence](https://github.com/PabloAcuaviva/visual-intelligence) |
| [Video models are zero-shot learners and reasoners](https://arxiv.org/search/?query=Video+models+are+zero-shot+learners+and+reasoners&searchtype=all&source=header) | 2025-09 | N/A |
| [VMEvalKit](https://arxiv.org/search/?query=VMEvalKit&searchtype=all&source=header) | 2024-07 | [open-compass/VLMEvalKit](https://github.com/open-compass/VLMEvalKit) |

## Benchmarks
_List benchmarks with key metrics, task coverage, and official evaluation scripts._
- TBD

## Datasets
_Describe dataset scale, modality, licensing, and download links._
- TBD

## Frameworks & Tooling
_Highlight libraries, agents, or reference implementations for video understanding and decision-making._
- TBD

## Tutorials & Courses
_Add learning materials such as blog posts, tutorials, and lecture notes._
- TBD

## Contributing
Contributions are welcome! To add a resource:
1. Open a pull request with a clear title and short description of the change.
2. Place new entries in the appropriate section, sorted newest-first where applicable.
3. Include links to papers, code, and official assets, and note licenses when relevant.

## License
This project is licensed under the [MIT License](LICENSE) unless otherwise noted by individual resources.
